"""
AIåŠ©æ‰‹æ¨¡å—ä¸šåŠ¡é€»è¾‘ Service
æ”¯æŒæ··åˆæ¨¡å¼ï¼šæœ¬åœ° Llama-cpp æ¨ç† + åœ¨çº¿ API (OpenAI å…¼å®¹æ ¼å¼)
æ”¯æŒå¤šè§’è‰²é¢„è®¾ã€çŸ¥è¯†åº“RAGã€æ•°æ®åˆ†æåŠ©æ‰‹
"""

import os
import re
import json
import logging
import asyncio
import httpx
from typing import List, Dict, Any, Optional, Generator, Union
from llama_cpp import Llama

from core.config import get_settings
from utils.storage import get_storage_manager

logger = logging.getLogger(__name__)
storage_manager = get_storage_manager()

class AIService:
    _model_instances = {}
    
    # è§’è‰²é¢„è®¾å®šä¹‰ï¼ˆä¸å‰ç«¯AIPage.ROLE_PRESETSå¯¹åº”ï¼‰
    # default=é€šç”¨åŠ©æ‰‹, coder=ç¼–ç¨‹åŠ©æ‰‹, writer=å†™ä½œåŠ©æ‰‹, translator=ç¿»è¯‘åŠ©æ‰‹, analyst=æ•°æ®åŠ©æ‰‹
    ROLE_PRESETS = {
        'default': 'ä½ æ˜¯ä¸€ä¸ªå…¨èƒ½æ™ºèƒ½åŠ©æ‰‹ã€‚',
        'coder': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿å¤šç§ç¼–ç¨‹è¯­è¨€å’Œæ¡†æ¶ã€‚è¯·æä¾›æ¸…æ™°ã€é«˜æ•ˆã€å¯ç»´æŠ¤çš„ä»£ç è§£å†³æ–¹æ¡ˆï¼Œå¹¶é™„å¸¦å¿…è¦çš„ä»£ç æ³¨é‡Šã€‚',
        'writer': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†™ä½œåŠ©æ‰‹ï¼Œæ“…é•¿å„ç§æ–‡ä½“é£æ ¼ã€‚è¯·å¸®åŠ©æˆ‘åˆ›ä½œã€ä¿®æ”¹å’Œæ”¹è¿›æ–‡å­—å†…å®¹ï¼Œç¡®ä¿è¯­è¨€æµç•…ã€æ¡ç†æ¸…æ™°ã€‚',
        'translator': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ï¼Œç²¾é€šä¸­è‹±åŒè¯­ç¿»è¯‘ã€‚è¯·å¸®åŠ©æˆ‘ç¿»è¯‘æ–‡æœ¬ï¼Œä¿æŒåŸæ–‡çš„è¯­æ°”å’Œé£æ ¼ï¼ŒåŒæ—¶ç¡®ä¿è¯‘æ–‡è‡ªç„¶é€šé¡ºã€‚',
        'analyst': 'ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ï¼Œæ“…é•¿SQLã€Pythonå’Œæ•°æ®å¯è§†åŒ–ã€‚è¯·å¸®åŠ©æˆ‘åˆ†ææ•°æ®å¹¶æä¾›æ·±å…¥çš„æ´å¯Ÿå’Œå»ºè®®ã€‚'
    }
    
    # é»˜è®¤æ¨¡å‹
    DEFAULT_MODEL = "qwen2.5-coder-7b-instruct-q4_k_m.gguf"
    
    @classmethod
    def get_available_models(cls) -> List[str]:
        """è·å–å¯ç”¨çš„æœ¬åœ°æ¨¡å‹åˆ—è¡¨"""
        models_dir = cls.get_model_path("")
        if not os.path.exists(models_dir):
            return []
        return [f for f in os.listdir(models_dir) if f.endswith(".gguf")]
    
    @classmethod
    def get_model_path(cls, model_filename: str = "qwen2.5-coder-7b-instruct-q4_k_m.gguf") -> str:
        """è·å–æœ¬åœ°æ¨¡å‹ç»å¯¹è·¯å¾„"""
        # æ¨¡å‹å­˜æ”¾äº storage/modules/ai/ai_models/
        models_dir = storage_manager.get_module_dir("ai", "ai_models")
        if not model_filename:
            return str(models_dir)
        return str(models_dir / model_filename)

    # è®°å½•åŠ è½½å¤±è´¥çš„æ¨¡å‹ï¼Œé¿å…é‡å¤å°è¯•
    _failed_models = set()
    
    @classmethod
    def _get_llm(cls, model_filename: str) -> Llama:
        """è·å–æˆ–åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹å®ä¾‹"""
        # æ£€æŸ¥æ˜¯å¦å·²çŸ¥åŠ è½½å¤±è´¥çš„æ¨¡å‹
        if model_filename in cls._failed_models:
            raise RuntimeError(f"æ¨¡å‹ {model_filename} ä¹‹å‰åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æˆ–é€‰æ‹©å…¶ä»–æ¨¡å‹")
        
        if model_filename not in cls._model_instances:
            model_path = cls.get_model_path(model_filename)
            
            if not os.path.exists(model_path):
                logger.error(f"æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆè­¦å‘Šå¤§æ¨¡å‹ï¼‰
            file_size_gb = os.path.getsize(model_path) / (1024 ** 3)
            if file_size_gb > 10:
                logger.warning(f"æ¨¡å‹æ–‡ä»¶è¾ƒå¤§ ({file_size_gb:.1f}GB)ï¼ŒåŠ è½½å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´å’Œå¤§é‡å†…å­˜")
            
            try:
                logger.info(f"æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {model_path}...")
                cls._model_instances[model_filename] = Llama(
                    model_path=model_path,
                    n_ctx=2048,
                    n_threads=os.cpu_count(),
                    n_gpu_layers=0,
                    verbose=False
                )
                logger.info("æœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆ")
            except Exception as e:
                # è®°å½•å¤±è´¥çš„æ¨¡å‹
                cls._failed_models.add(model_filename)
                error_msg = str(e)
                
                # æä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
                if "Failed to load model" in error_msg:
                    logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {model_filename}")
                    logger.error("å¯èƒ½åŸå› : 1) æ¨¡å‹æ–‡ä»¶æŸå 2) å†…å­˜ä¸è¶³ 3) æ¨¡å‹æ ¼å¼ä¸å…¼å®¹")
                    raise RuntimeError(f"æ¨¡å‹ {model_filename} åŠ è½½å¤±è´¥ï¼Œå¯èƒ½æ˜¯æ–‡ä»¶æŸåã€å†…å­˜ä¸è¶³æˆ–æ ¼å¼ä¸å…¼å®¹ã€‚è¯·å°è¯•ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ã€‚")
                else:
                    logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {error_msg}")
                    raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {error_msg}")
            
        return cls._model_instances[model_filename]

    @classmethod
    async def _chat_local(cls, messages: List[Dict[str, str]], stream: bool = True, model_name: Optional[str] = None) -> Any:
        """æœ¬åœ°æ¨¡å‹æ¨ç†"""
        loop = asyncio.get_event_loop()

        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆæ’é™¤å·²çŸ¥å¤±è´¥çš„æ¨¡å‹ï¼‰
        available = [m for m in cls.get_available_models() if m not in cls._failed_models]
        
        if not available:
            if cls._failed_models:
                raise RuntimeError(f"æ‰€æœ‰æ¨¡å‹åŠ è½½å¤±è´¥ã€‚å¤±è´¥çš„æ¨¡å‹: {', '.join(cls._failed_models)}ã€‚è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æˆ–ä¸‹è½½æ–°çš„æ¨¡å‹ã€‚")
            else:
                raise FileNotFoundError("æœªæ‰¾åˆ°å¯ç”¨çš„æœ¬åœ°æ¨¡å‹ï¼Œè¯·å°†.ggufæ¨¡å‹æ–‡ä»¶æ”¾ç½®åˆ°storage/modules/ai/ai_models/ç›®å½•")

        # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹
        if model_name and model_name in cls._failed_models:
            # å¦‚æœæŒ‡å®šçš„æ¨¡å‹å·²çŸ¥å¤±è´¥ï¼Œé€‰æ‹©å…¶ä»–å¯ç”¨æ¨¡å‹
            logger.warning(f"æŒ‡å®šçš„æ¨¡å‹ {model_name} å·²çŸ¥åŠ è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å…¶ä»–æ¨¡å‹")
            model_name = None
        
        if not model_name:
            # ä¼˜å…ˆä½¿ç”¨é»˜è®¤æ¨¡å‹
            if cls.DEFAULT_MODEL in available:
                model_name = available[0] if cls.DEFAULT_MODEL in cls._failed_models else cls.DEFAULT_MODEL
            else:
                model_name = available[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹

        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½ (åœ¨çº¿ç¨‹ä¸­åŠ è½½ä»¥é˜²é˜»å¡)
        try:
            if model_name not in cls._model_instances:
                await loop.run_in_executor(None, cls._get_llm, model_name)
            llm = cls._get_llm(model_name)
        except Exception as e:
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œå°è¯•å…¶ä»–å¯ç”¨æ¨¡å‹
            logger.warning(f"æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ¨¡å‹...")
            other_models = [m for m in available if m != model_name and m not in cls._failed_models]
            for fallback_model in other_models:
                try:
                    logger.info(f"å°è¯•åŠ è½½å¤‡é€‰æ¨¡å‹: {fallback_model}")
                    if fallback_model not in cls._model_instances:
                        await loop.run_in_executor(None, cls._get_llm, fallback_model)
                    llm = cls._get_llm(fallback_model)
                    model_name = fallback_model
                    break
                except Exception:
                    continue
            else:
                # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†
                raise RuntimeError(f"æ— æ³•åŠ è½½ä»»ä½•æœ¬åœ°æ¨¡å‹ã€‚è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æˆ–ä½¿ç”¨åœ¨çº¿æ¨¡å¼ã€‚åŸå§‹é”™è¯¯: {e}")

        def _create_generator():
            return llm.create_chat_completion(
                messages=messages,
                stream=stream,
                temperature=0.7,
                max_tokens=2048
            )

        # è·å–åŒæ­¥è¿­ä»£å™¨
        sync_iterator = await loop.run_in_executor(None, _create_generator)

        # è½¬æ¢ä¸ºå¼‚æ­¥è¿­ä»£å™¨ï¼Œå¹¶åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œè®¡ç®—å¯†é›†å‹çš„ next()
        async def _async_generator():
            # å®šä¹‰ä¸€ä¸ªå¸¦æœ‰å¼‚å¸¸æ•è·çš„è¿­ä»£è¾…åŠ©å‡½æ•°
            def _get_next():
                try:
                    return next(sync_iterator)
                except StopIteration:
                    return None # ä½¿ç”¨ None ä½œä¸ºç»“æŸä¿¡å·
            
            while True:
                try:
                    # åœ¨çº¿ç¨‹æ± ä¸­è·å–ä¸‹ä¸€ä¸ª token
                    chunk = await loop.run_in_executor(None, _get_next)
                    if chunk is None:
                        break
                    yield chunk
                except Exception as e:
                    logger.error(f"Local inference error: {e}")
                    break
        
        return _async_generator()

    @classmethod
    async def _chat_online(cls, messages: List[Dict[str, str]], stream: bool = True, api_config: Optional[Dict[str, str]] = None) -> Any:
        """åœ¨çº¿ API æ¨ç† (OpenAI å…¼å®¹æ ¼å¼)"""
        settings = get_settings()
        
        # ä¼˜å…ˆä½¿ç”¨å‰ç«¯ä¼ å…¥çš„é…ç½®
        if api_config:
            api_key = api_config.get("apiKey")
            base_url = api_config.get("baseUrl")
            model_name = api_config.get("model")
        else:
            api_key = getattr(settings, "ai_online_api_key", "sk-xxx")
            base_url = getattr(settings, "ai_online_base_url", "https://api.deepseek.com/v1")
            model_name = getattr(settings, "ai_online_model", "deepseek-chat")

        if not api_key or api_key == "sk-xxx":
            raise ValueError("æœªé…ç½®åœ¨çº¿ API Key")

        async def generator():
            async with httpx.AsyncClient(timeout=60.0) as client:
                async with client.stream(
                    "POST",
                    f"{base_url}/chat/completions",
                    headers={"Authorization": f"Bearer {api_key}"},
                    json={
                        "model": model_name,
                        "messages": messages,
                        "stream": stream,
                        "temperature": 0.7
                    }
                ) as response:
                    if response.status_code != 200:
                        error_text = await response.aread()
                        raise Exception(f"åœ¨çº¿ API è¯·æ±‚å¤±è´¥ ({response.status_code}): {error_text.decode()}")
                    
                    async for line in response.aiter_lines():
                        if not line or not line.startswith("data: "):
                            continue
                        
                        data_str = line[6:].strip()
                        if data_str == "[DONE]":
                            break
                        
                        try:
                            chunk = json.loads(data_str)
                            yield chunk
                        except:
                            continue

        return generator() if stream else await cls._chat_online_sync(messages)

    @classmethod
    async def _chat_online_sync(cls, messages: List[Dict[str, str]]) -> Dict:
        # ç®€åŒ–ç‰ˆåŒæ­¥è°ƒç”¨
        return {"choices": [{"message": {"content": "åœ¨çº¿åŒæ­¥æ¨¡å¼æš‚æœªå®Œå…¨å®ç°"}}]}

    @classmethod
    def _is_data_analysis_query(cls, query: str) -> bool:
        """æ£€æµ‹é—®é¢˜æ˜¯å¦æ¶‰åŠæ•°æ®åˆ†æ"""
        query_lower = query.lower()
        # æ•°æ®åˆ†æç›¸å…³çš„å…³é”®è¯
        data_keywords = [
            'æ•°æ®', 'dataset', 'æ•°æ®é›†', 'ç»Ÿè®¡', 'åˆ†æ', 'æŸ¥è¯¢', 'sql',
            'è¡¨æ ¼', 'è¡¨', 'åˆ—', 'è¡Œ', 'å­—æ®µ', 'æ±‡æ€»', 'èšåˆ', 'å¹³å‡å€¼',
            'æœ€å¤§å€¼', 'æœ€å°å€¼', 'æ€»å’Œ', 'è®¡æ•°', 'å›¾è¡¨', 'å¯è§†åŒ–',
            'excel', 'csv', 'å¯¼å…¥', 'å¯¼å‡º', 'æ¸…æ´—', 'å»ºæ¨¡',
            'æ‰¾å‡º', 'æ˜¾ç¤º', 'åˆ—å‡º', 'è·å–', 'ç­›é€‰', 'è¿‡æ»¤'
        ]
        return any(keyword in query_lower for keyword in data_keywords)

    @classmethod
    def _generate_sql_from_natural_language(cls, query: str, dataset, columns: List[str] = None) -> Optional[str]:
        """ä»è‡ªç„¶è¯­è¨€ç”ŸæˆSQLæŸ¥è¯¢"""
        try:
            query_lower = query.lower()
            table_name = dataset.table_name
            
            # é»˜è®¤é€‰æ‹©æ‰€æœ‰åˆ—
            select_clause = "*"
            conditions = []
            limit_clause = ""
            order_clause = ""
            group_clause = ""
            
            # å¦‚æœæœ‰åˆ—ä¿¡æ¯ï¼Œå°è¯•æ›´æ™ºèƒ½åœ°åŒ¹é…
            if columns:
                columns_lower = [c.lower() for c in columns]
                
                # æ£€æµ‹èšåˆå‡½æ•°éœ€æ±‚
                if any(kw in query_lower for kw in ['æ€»æ•°', 'æ•°é‡', 'è®¡æ•°', 'count', 'å¤šå°‘']):
                    select_clause = "COUNT(*) as æ•°é‡"
                elif any(kw in query_lower for kw in ['å¹³å‡', 'average', 'avg', 'å‡å€¼']):
                    # æ‰¾åˆ°æ•°å€¼ç±»å‹çš„åˆ—
                    for col in columns:
                        if any(kw in col.lower() for kw in ['amount', 'price', 'count', 'num', 'qty', 'value', 'é‡‘é¢', 'æ•°é‡', 'ä»·æ ¼']):
                            select_clause = f"AVG({col}) as å¹³å‡å€¼"
                            break
                elif any(kw in query_lower for kw in ['æ€»å’Œ', 'åˆè®¡', 'sum', 'æ€»è®¡']):
                    for col in columns:
                        if any(kw in col.lower() for kw in ['amount', 'price', 'count', 'num', 'qty', 'value', 'é‡‘é¢', 'æ•°é‡', 'ä»·æ ¼']):
                            select_clause = f"SUM({col}) as æ€»è®¡"
                            break
                elif any(kw in query_lower for kw in ['æœ€å¤§', 'max', 'æœ€é«˜']):
                    for col in columns:
                        if any(kw in col.lower() for kw in ['amount', 'price', 'count', 'num', 'qty', 'value', 'é‡‘é¢', 'æ•°é‡', 'ä»·æ ¼']):
                            select_clause = f"MAX({col}) as æœ€å¤§å€¼, *"
                            order_clause = f" ORDER BY {col} DESC"
                            break
                elif any(kw in query_lower for kw in ['æœ€å°', 'min', 'æœ€ä½']):
                    for col in columns:
                        if any(kw in col.lower() for kw in ['amount', 'price', 'count', 'num', 'qty', 'value', 'é‡‘é¢', 'æ•°é‡', 'ä»·æ ¼']):
                            select_clause = f"MIN({col}) as æœ€å°å€¼, *"
                            order_clause = f" ORDER BY {col} ASC"
                            break
                
                # æ£€æµ‹åˆ†ç»„éœ€æ±‚
                if any(kw in query_lower for kw in ['æŒ‰', 'åˆ†ç»„', 'group', 'æ¯ä¸ª', 'å„ä¸ª']):
                    for col in columns:
                        col_lower = col.lower()
                        if any(kw in col_lower for kw in ['category', 'type', 'status', 'ç±»åˆ«', 'ç±»å‹', 'çŠ¶æ€', 'name', 'åç§°']):
                            group_clause = f" GROUP BY {col}"
                            if select_clause == "*":
                                select_clause = f"{col}, COUNT(*) as æ•°é‡"
                            break
            
            # æ£€æµ‹é™åˆ¶æ•°é‡
            limit_match = re.search(r'(å‰|top|limit|æœ€å¤š|åªæ˜¾ç¤º)\s*(\d+)', query_lower)
            if limit_match:
                limit_num = int(limit_match.group(2))
                limit_clause = f" LIMIT {min(limit_num, 100)}"
            elif 'å‰10' in query_lower or 'å‰å' in query_lower:
                limit_clause = " LIMIT 10"
            elif 'å‰20' in query_lower or 'å‰äºŒå' in query_lower:
                limit_clause = " LIMIT 20"
            elif 'å‰5' in query_lower or 'å‰äº”' in query_lower:
                limit_clause = " LIMIT 5"
            else:
                limit_clause = " LIMIT 100"  # é»˜è®¤é™åˆ¶
            
            # æ£€æµ‹æ’åºï¼ˆå¦‚æœè¿˜æ²¡æœ‰è®¾ç½®ï¼‰
            if not order_clause:
                if 'æœ€å¤§' in query_lower or 'æœ€é«˜' in query_lower or 'æœ€å¤š' in query_lower:
                    order_clause = " ORDER BY 1 DESC"
                elif 'æœ€å°' in query_lower or 'æœ€ä½' in query_lower or 'æœ€å°‘' in query_lower:
                    order_clause = " ORDER BY 1 ASC"
                elif 'æœ€æ–°' in query_lower or 'æœ€è¿‘' in query_lower:
                    if columns:
                        for col in columns:
                            if any(kw in col.lower() for kw in ['date', 'time', 'created', 'updated', 'æ—¥æœŸ', 'æ—¶é—´']):
                                order_clause = f" ORDER BY {col} DESC"
                                break
            
            # ç»„åˆSQL
            sql = f"SELECT {select_clause} FROM {table_name}"
            if conditions:
                sql += " WHERE " + " AND ".join(conditions)
            if group_clause:
                sql += group_clause
            if order_clause:
                sql += order_clause
            sql += limit_clause
            
            return sql
        except Exception as e:
            logger.warning(f"ç”ŸæˆSQLå¤±è´¥: {e}")
            return None

    @classmethod
    def _suggest_visualization(cls, query: str, columns: List[str] = None, data_sample: List[dict] = None) -> str:
        """æ ¹æ®æŸ¥è¯¢å’Œæ•°æ®ç‰¹å¾æ¨èå¯è§†åŒ–ç±»å‹"""
        query_lower = query.lower()
        suggestions = []
        
        # åŸºäºæŸ¥è¯¢å…³é”®è¯çš„å»ºè®®
        if any(kw in query_lower for kw in ['è¶‹åŠ¿', 'å˜åŒ–', 'æ—¶é—´', 'å†å²', 'trend']):
            suggestions.append("ğŸ“ˆ **æŠ˜çº¿å›¾**ï¼šé€‚åˆå±•ç¤ºæ•°æ®éšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿")
        
        if any(kw in query_lower for kw in ['å æ¯”', 'æ¯”ä¾‹', 'åˆ†å¸ƒ', 'ç™¾åˆ†æ¯”', 'pie']):
            suggestions.append("ğŸ¥§ **é¥¼å›¾**ï¼šé€‚åˆå±•ç¤ºå„éƒ¨åˆ†å æ€»ä½“çš„æ¯”ä¾‹")
        
        if any(kw in query_lower for kw in ['å¯¹æ¯”', 'æ¯”è¾ƒ', 'æ’å', 'top', 'å‰', 'compare']):
            suggestions.append("ğŸ“Š **æŸ±çŠ¶å›¾**ï¼šé€‚åˆå¯¹æ¯”ä¸åŒç±»åˆ«çš„æ•°å€¼å¤§å°")
        
        if any(kw in query_lower for kw in ['æ•£ç‚¹', 'ç›¸å…³', 'å…³ç³»', 'scatter', 'correlation']):
            suggestions.append("â­• **æ•£ç‚¹å›¾**ï¼šé€‚åˆåˆ†æä¸¤ä¸ªå˜é‡ä¹‹é—´çš„å…³ç³»")
        
        if any(kw in query_lower for kw in ['çƒ­åŠ›', 'çŸ©é˜µ', 'çƒ­åº¦', 'heatmap']):
            suggestions.append("ğŸ”¥ **çƒ­åŠ›å›¾**ï¼šé€‚åˆå±•ç¤ºå¤šç»´åº¦æ•°æ®çš„å¼ºåº¦åˆ†å¸ƒ")
        
        # åŸºäºæ•°æ®ç‰¹å¾çš„å»ºè®®
        if columns:
            has_date = any(kw in col.lower() for col in columns for kw in ['date', 'time', 'æ—¥æœŸ', 'æ—¶é—´'])
            has_category = any(kw in col.lower() for col in columns for kw in ['category', 'type', 'status', 'ç±»åˆ«', 'ç±»å‹'])
            has_numeric = any(kw in col.lower() for col in columns for kw in ['amount', 'price', 'count', 'é‡‘é¢', 'æ•°é‡'])
            
            if has_date and has_numeric and not suggestions:
                suggestions.append("ğŸ“ˆ **æŠ˜çº¿å›¾**ï¼šæ£€æµ‹åˆ°æ—¶é—´å’Œæ•°å€¼å­—æ®µï¼Œé€‚åˆå±•ç¤ºæ—¶é—´è¶‹åŠ¿")
            elif has_category and has_numeric and not suggestions:
                suggestions.append("ğŸ“Š **æŸ±çŠ¶å›¾**ï¼šæ£€æµ‹åˆ°åˆ†ç±»å’Œæ•°å€¼å­—æ®µï¼Œé€‚åˆå¯¹æ¯”åˆ†æ")
        
        if not suggestions:
            suggestions.append("ğŸ“Š **æŸ±çŠ¶å›¾**ï¼šé€šç”¨çš„æ•°æ®å¯¹æ¯”å±•ç¤ºæ–¹å¼")
            suggestions.append("ğŸ“ˆ **æŠ˜çº¿å›¾**ï¼šå¦‚æœæ•°æ®æœ‰æ—¶é—´ç»´åº¦ï¼Œå¯ä»¥å°è¯•")
        
        return "\n".join(suggestions)

    @classmethod
    async def _get_analysis_context(cls, query: str) -> str:
        """è·å–æ•°æ®åˆ†æä¸Šä¸‹æ–‡"""
        try:
            from core.database import async_session
            from modules.analysis.analysis_models import AnalysisDataset
            from modules.analysis.analysis_modeling_service import ModelingService
            from sqlalchemy import select
            import re
            
            context_parts = []
            
            async with async_session() as db:
                # 1. è·å–æ•°æ®é›†åˆ—è¡¨
                result = await db.execute(
                    select(AnalysisDataset).order_by(AnalysisDataset.updated_at.desc()).limit(10)
                )
                datasets = result.scalars().all()
                
                if datasets:
                    context_parts.append("\n--- å¯ç”¨æ•°æ®é›† ---")
                    for ds in datasets:
                        context_parts.append(f"- æ•°æ®é›†ID {ds.id}: {ds.name} ({ds.row_count or 0} è¡Œ, è¡¨å: {ds.table_name})")
                
                # 2. æ£€æµ‹å¹¶æ‰§è¡Œ SQL æŸ¥è¯¢
                query_lower = query.lower()
                
                # æ£€æµ‹æ˜¯å¦åŒ…å« SQL è¯­å¥
                sql_match = re.search(r'select\s+.*?\s+from\s+(\w+)', query_lower, re.IGNORECASE)
                if sql_match:
                    table_name = sql_match.group(1)
                    # æŸ¥æ‰¾å¯¹åº”çš„æ•°æ®é›†
                    dataset = None
                    for ds in datasets:
                        if ds.table_name == table_name or str(ds.id) == table_name:
                            dataset = ds
                            break
                    
                    if dataset:
                        try:
                            # æå–å®Œæ•´çš„ SQL è¯­å¥
                            sql_full_match = re.search(r'(select\s+.*?\s+from\s+\w+.*?)(?:[ã€‚ï¼Œ,\.\n]|$)', query, re.IGNORECASE | re.DOTALL)
                            if sql_full_match:
                                sql_query = sql_full_match.group(1).strip()
                                # æ›¿æ¢è¡¨åä¸ºå®é™…è¡¨å
                                sql_query = re.sub(r'from\s+\w+', f'FROM {dataset.table_name}', sql_query, flags=re.IGNORECASE)
                                
                                # æ‰§è¡Œ SQL æŸ¥è¯¢
                                sql_result = await ModelingService.execute_sql(db, sql_query, limit=100)
                                context_parts.append(f"\n--- SQL æŸ¥è¯¢ç»“æœ (æ•°æ®é›†: {dataset.name}) ---")
                                context_parts.append(f"æŸ¥è¯¢: {sql_query}")
                                context_parts.append(f"è¿”å› {sql_result['row_count']} è¡Œæ•°æ®")
                                if sql_result['row_count'] > 0:
                                    # åªæ˜¾ç¤ºå‰5è¡Œä½œä¸ºç¤ºä¾‹
                                    sample_rows = sql_result['rows'][:5]
                                    context_parts.append(f"åˆ—: {', '.join(sql_result['columns'])}")
                                    context_parts.append(f"ç¤ºä¾‹æ•°æ® (å‰5è¡Œ): {json.dumps(sample_rows, ensure_ascii=False, indent=2)}")
                                    if sql_result['row_count'] > 5:
                                        context_parts.append(f"(å…± {sql_result['row_count']} è¡Œï¼Œä»…æ˜¾ç¤ºå‰5è¡Œ)")
                        except Exception as e:
                            logger.warning(f"æ‰§è¡Œ SQL æŸ¥è¯¢å¤±è´¥: {e}")
                            context_parts.append(f"\nSQL æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}")
                
                # 3. è‡ªç„¶è¯­è¨€è½¬SQLï¼ˆå¦‚æœé—®é¢˜åŒ…å«æŸ¥è¯¢æ„å›¾ä½†æ²¡æœ‰æ˜ç¡®çš„SQLï¼‰
                elif any(kw in query_lower for kw in ['æŸ¥è¯¢', 'æ‰¾å‡º', 'æ˜¾ç¤º', 'åˆ—å‡º', 'è·å–', 'ç­›é€‰', 'è¿‡æ»¤', 'ç»Ÿè®¡', 'åˆè®¡', 'æ€»æ•°', 'å¹³å‡']) and 'select' not in query_lower:
                    # å°è¯•ä»é—®é¢˜ä¸­æå–æ•°æ®é›†ä¿¡æ¯
                    dataset_id_match = re.search(r'æ•°æ®é›†[ï¼š:]?\s*(\d+)', query)
                    dataset_name_match = re.search(r'æ•°æ®é›†[ï¼š:]?\s*([^\sï¼Œ,ã€‚.]+)', query)
                    
                    dataset_id = None
                    if dataset_id_match:
                        dataset_id = int(dataset_id_match.group(1))
                    elif dataset_name_match and datasets:
                        dataset_name = dataset_name_match.group(1)
                        for ds in datasets:
                            if dataset_name in ds.name:
                                dataset_id = ds.id
                                break
                    elif datasets:
                        # å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œä½¿ç”¨æœ€æ–°çš„æ•°æ®é›†
                        dataset_id = datasets[0].id
                    
                    if dataset_id:
                        dataset = next((ds for ds in datasets if ds.id == dataset_id), None)
                        if dataset:
                            try:
                                # è·å–æ•°æ®é›†çš„åˆ—ä¿¡æ¯
                                columns = []
                                try:
                                    col_result = await ModelingService.execute_sql(
                                        db, 
                                        f"SELECT * FROM {dataset.table_name} LIMIT 1",
                                        limit=1
                                    )
                                    columns = col_result.get('columns', [])
                                except:
                                    pass
                                
                                # ä½¿ç”¨å¢å¼ºçš„SQLç”Ÿæˆ
                                sql_suggestion = cls._generate_sql_from_natural_language(query, dataset, columns)
                                if sql_suggestion:
                                    try:
                                        # æ‰§è¡Œç”Ÿæˆçš„SQL
                                        sql_result = await ModelingService.execute_sql(db, sql_suggestion, limit=100)
                                        context_parts.append(f"\n--- è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç»“æœ (æ•°æ®é›†: {dataset.name}) ---")
                                        context_parts.append(f"ç”Ÿæˆçš„SQL: {sql_suggestion}")
                                        context_parts.append(f"è¿”å› {sql_result['row_count']} è¡Œæ•°æ®")
                                        if sql_result['row_count'] > 0:
                                            sample_rows = sql_result['rows'][:5]
                                            context_parts.append(f"åˆ—: {', '.join(sql_result['columns'])}")
                                            context_parts.append(f"ç¤ºä¾‹æ•°æ® (å‰5è¡Œ): {json.dumps(sample_rows, ensure_ascii=False, indent=2)}")
                                            if sql_result['row_count'] > 5:
                                                context_parts.append(f"(å…± {sql_result['row_count']} è¡Œï¼Œä»…æ˜¾ç¤ºå‰5è¡Œ)")
                                            
                                            # æ·»åŠ å¯è§†åŒ–å»ºè®®
                                            viz_suggestion = cls._suggest_visualization(query, sql_result['columns'])
                                            context_parts.append(f"\n--- å¯è§†åŒ–å»ºè®® ---")
                                            context_parts.append(viz_suggestion)
                                    except Exception as e:
                                        logger.warning(f"æ‰§è¡Œç”Ÿæˆçš„SQLå¤±è´¥: {e}")
                                        context_parts.append(f"\nSQLç”Ÿæˆå»ºè®®: {sql_suggestion}")
                                        context_parts.append(f"æ‰§è¡Œå¤±è´¥: {str(e)}")
                            except Exception as e:
                                logger.warning(f"è‡ªç„¶è¯­è¨€è½¬SQLå¤„ç†å¤±è´¥: {e}")
                
                # 4. å¦‚æœé—®é¢˜åŒ…å«ç»Ÿè®¡ã€æ±‡æ€»ç­‰å…³é”®è¯ï¼Œå°è¯•è·å–ç»Ÿè®¡ä¿¡æ¯
                elif any(kw in query_lower for kw in ['ç»Ÿè®¡', 'æ±‡æ€»', 'åˆ†æ', 'æè¿°', 'summary']):
                    # å°è¯•ä»é—®é¢˜ä¸­æå–æ•°æ®é›†IDæˆ–åç§°
                    dataset_id_match = re.search(r'æ•°æ®é›†[ï¼š:]\s*(\d+)', query)
                    dataset_name_match = re.search(r'æ•°æ®é›†[ï¼š:]\s*([^\sï¼Œ,ã€‚.]+)', query)
                    
                    dataset_id = None
                    if dataset_id_match:
                        dataset_id = int(dataset_id_match.group(1))
                    elif dataset_name_match and datasets:
                        dataset_name = dataset_name_match.group(1)
                        for ds in datasets:
                            if dataset_name in ds.name:
                                dataset_id = ds.id
                                break
                    elif datasets:
                        # å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œä½¿ç”¨æœ€æ–°çš„æ•°æ®é›†
                        dataset_id = datasets[0].id
                    
                    # å¦‚æœæ‰¾åˆ°äº†æ•°æ®é›†ï¼Œè·å–ç»Ÿè®¡ä¿¡æ¯
                    if dataset_id:
                        try:
                            summary = await ModelingService.get_summary(db, dataset_id)
                            dataset = next((ds for ds in datasets if ds.id == dataset_id), None)
                            context_parts.append(f"\n--- æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ ({dataset.name if dataset else f'ID {dataset_id}'}) ---")
                            # ç®€åŒ–ç»Ÿè®¡ä¿¡æ¯è¾“å‡º
                            if 'stats' in summary:
                                stats = summary['stats']
                                context_parts.append(f"æ•°æ®é›†åŒ…å« {len(stats)} ä¸ªå­—æ®µçš„ç»Ÿè®¡ä¿¡æ¯")
                                # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡æŒ‡æ ‡
                                for col, col_stats in list(stats.items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªå­—æ®µ
                                    if isinstance(col_stats, dict):
                                        context_parts.append(f"  {col}: {json.dumps(col_stats, ensure_ascii=False)}")
                            if 'missing' in summary:
                                missing = summary['missing']
                                total_missing = sum(missing.values())
                                if total_missing > 0:
                                    context_parts.append(f"ç¼ºå¤±å€¼ç»Ÿè®¡: å…± {total_missing} ä¸ªç¼ºå¤±å€¼")
                        except Exception as e:
                            logger.warning(f"è·å–æ•°æ®é›†ç»Ÿè®¡å¤±è´¥: {e}")
            
            return "\n".join(context_parts) if context_parts else ""
        except Exception as e:
            logger.error(f"è·å–æ•°æ®åˆ†æä¸Šä¸‹æ–‡å¤±è´¥: {e}", exc_info=True)
            return ""

    @classmethod
    async def chat_with_context(
        cls, 
        query: str, 
        history: List[Dict[str, str]] = [],
        knowledge_base_id: Optional[int] = None,
        use_analysis: bool = False,
        provider: str = "local", # "local" æˆ– "online"
        role_preset: str = "default",  # è§’è‰²é¢„è®¾
        model_name: Optional[str] = None,  # æœ¬åœ°æ¨¡å‹åç§°
        api_config: Optional[Dict[str, str]] = None
    ) -> Any:
        """å¸¦æœ‰ä¸Šä¸‹æ–‡çš„æ··åˆæ¨¡å¼å¯¹è¯"""
        
        context = ""
        
        # 1. çŸ¥è¯†åº“é›†æˆ (RAG)
        if knowledge_base_id:
            try:
                from modules.knowledge.knowledge_services import KnowledgeService
                from core.database import async_session
                async with async_session() as db:
                    search_results = await KnowledgeService.search(db, knowledge_base_id, query)
                    if search_results:
                        context += "\n--- å‚è€ƒçŸ¥è¯†åº“èµ„æ–™ ---\n"
                        for res in search_results:
                            context += f"- {res['content']}\n"
            except Exception as e:
                logger.error(f"RAG search error: {e}")

        # 2. æ•°æ®åˆ†æåŠ©æ‰‹é›†æˆ
        if use_analysis and cls._is_data_analysis_query(query):
            try:
                analysis_context = await cls._get_analysis_context(query)
                if analysis_context:
                    context += "\n--- æ•°æ®åˆ†æåŠ©æ‰‹ä¿¡æ¯ ---"
                    context += analysis_context
                    context += "\n\nä½ å¯ä»¥ä½¿ç”¨è¿™äº›æ•°æ®é›†ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
            except Exception as e:
                logger.error(f"æ•°æ®åˆ†æåŠ©æ‰‹é”™è¯¯: {e}", exc_info=True)

        # 3. æ„é€ æ¶ˆæ¯é˜Ÿåˆ—
        # ä½¿ç”¨è§’è‰²é¢„è®¾ç¡®å®šç³»ç»Ÿæç¤ºè¯
        sys_prompt = cls.ROLE_PRESETS.get(role_preset, cls.ROLE_PRESETS['default'])
        
        if use_analysis:
            sys_prompt += " ä½ å…·å¤‡æ•°æ®åˆ†æèƒ½åŠ›ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·æŸ¥è¯¢ã€åˆ†æå’Œç†è§£æ•°æ®ã€‚"
        
        if context:
            sys_prompt += f"\n\nä»¥ä¸‹æ˜¯ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯·ç»“åˆè¿™äº›ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š\n{context}"

        messages = [{"role": "system", "content": sys_prompt}]
        messages.extend(history)
        messages.append({"role": "user", "content": query})

        if provider == "online":
            return await cls._chat_online(messages, stream=True, api_config=api_config)
        else:
            return await cls._chat_local(messages, stream=True, model_name=model_name)
